<?xml version="1.0"?>
<!--
  Configuration file for CBOT.

  Note: Don't touch values of '0 0' in label 'perfhashstaticrandoms' if you don't know you are doing...
        These values must be setting to '0 0' for each change into relative group, after this, programs tell us the new numbers to put in.
		But you are free to leave it to '0 0' if don't want more stability at startup of programs.

1. MANDATORY SETTINGS
   You must set these before starting.

  collection/base
    The directory where the collection is stored.

  collection/maxdoc  collection/maxsite
    The maximum number of documents and sites,
    this is fixed and cannot be modified once
    the crawler has started! The actual maxima
    are 80% of this.

  seeder/link/accept/domain-suffixes
    The domain suffixes (country codes, domain names, etc.)
    that will be downloaded by the crawler.

  harvester/resolvconf
    The location of the dns servers

2. Recommended settings:

  harvester/nthreads
    The maximum number of simultaneous downloads

Number suffixes:
    K    = 1,000            Decimal kilo
    M    = 1,000,000        Decimal mega
    m    = minute
    h    = hour
    d    = day
    w    = week
    
 -->

<config>

    <!-- Parameters for storing the complete collection.
         A collection must reside completely in a directory.-->
    <collection>

        <!-- Number of distributed Storage.host.
             Min = 1 (it can be also localhost)
             default (1)-->
        <!-- *** IMPORTANT! If you use remote storage (like unix nfs filesystem), directories like metaidx0, metaidx1, metaidx2... must be created manually! *** -->
        <distributed>4</distributed>
            
        <!-- Base directory for storing everything in the collection.
             This directory is cleaned by 'cbot-bot-reset'; after
             this, it is possible to mount other filesystems
             in the sub-directories if more space is required.  -->
        <base>/tmp/cbotdata</base>

        <!-- These values are fixed in a collection. If you change these
             values, you must execute 'cbot-bot-reset' to delete
             the collection and re-generate the data files, so it is
             strongly recommended that you left enough space for growing.
             
             Typical values are:
              (.gr) 15000000 and 150000 
              (.es) 10000000 and 80000
              (.cl) 3500000 and 150000

         *** IMPORTANT! The maximum number actually allowed is 80% of this,
             so if you have 800 sites, you have to set maxsite to 1000 ***

             Note also that these defaults are VERY SMALL: 1,000,000 pages
             in 1,000 sites. They are good for a exploratory crawl but
             not for a large scale crawl (I've used up to 50M and 1M
             respectively)

         *** IMPORTANT! The REAL CAPACITY of index's crawler,
             is the product of maximum number and number of 'distributed'

         *** IMPORTANT! The maximum number of Second Level Domain
             include also some (italian) special domains in the hash list -->

        <maxdomain>500</maxdomain>
        <maxsite>500</maxsite>
        <maxdoc>50K</maxdoc>
        <maxword>50000</maxword>
        <maxstem>50000</maxstem>
        <maxngram>50000</maxngram>

	<sites-per-folder>400</sites-per-folder>

	<!-- Special domains for top_level_domain.-->
	<special-domains>
		<!-- domini italiani speciali di secondo livello e quelli di DUE caratteri sono TUTTI speciali ad esempio geografici -->
		<it-tld>
            <group label="perfhashstaticrandoms">0 0 0</group>
			<group label="tld">giustizia sassari difesa edu gov sardegna sardinia cagliari nuoro oristano</group>
		</it-tld>
	</special-domains>


	<!-- Note: Don't touch values of '0 0 0' in label 'perfhashstaticrandoms' if you don't know you are doing...
         These values MUST BE setting to '0 0 0' for each mew minimum variation into relative tag. After this, programs tell us the rigth numbers to put in.
		 But you are free to leave it to '0 0 0' if don't want more stability at startup of programs. -->
	<enable-perfhash-static-randoms>0</enable-perfhash-static-randoms>

    </collection>

    <!-- Parameters for the seeder program.
         The seeder program receives lists of urls obtained
         by the gatherer program, check if they have been seen before,
         and then decides what to do with them 
     -->
    <seeder>

        <!-- All the links that match these conditions will be
             added to the crawler's queue by default -->
        <accept>

            <!-- List of accepted protocols,
                 these must be supported by the harvester.
                 Protocol HTTPS is supported in beta version -->
            <protocol>
                <group label="perfhashstaticrandoms">0 0 0</group>
                <group label="proto">http https</group>
			</protocol>

            <!-- List of domain suffixes to accept for new links,
                 or list of top-level domains.
                 e.g.: ``.cl'' for Chile
                 Note that ALL sites
                 given as seeds will be accepted anyway, so this setting
                 applies only to new links.
                 Normally, all of them start with a dot.
                 This list is case-insensitive.
                 Cassiopeia use a list of full url like http://www.site.org/ -->
            <domain-suffixes>/tmp/cbotdata/domains.txt</domain-suffixes>
        </accept>

        <!-- Don't add more than this number of URLs per each site,
             note that most large Web sites have infinitely many URLs.
             Default: 10K -->

        <max-urls-per-site>10K</max-urls-per-site>

        <!-- This is to make some exceptions on the links that
             are accepted, based on the filename extension -->
        <extensions>

            <!-- Extensions that must be downloaded. -->
            <download>

                <!-- As all filename extensions are downloaded by default,
                     this list is not used during the crawling. It is
                     only used for reports at this moment. -->
                         
                <static>
                    <group label="document">xhtml:dhtml:html:htm  </group>
                </static>

                <!-- Files with a '?' in the url are marked as dynamic,
                     and they must also should be marked as dynamic if they
                     have one of these extensions.
                     Note that these files will be added, the crawler
                     supports dynamic pages. -->

                <dynamic>
                	<group label="perfhashstaticrandoms">0 0 0</group>
                    <group label="cgi">act dll shtml:shtm php:phtml:php5:php4:php3 cfm jsp jhtml asp:aspx:asx cgi pl tpl storefront py fcgi dyn bin zhtml htx d2w</group>
                </dynamic>
            </download>

            <!-- Files with these extensions must not be added to the
                 crawler's queue, they must be logged. The log includes the source docid where the
                 link was found, tag and position, the link to the file and the caption, if any -->
            <log>
                <group label="perfhashstaticrandoms">0 0 0</group>
                <group label="nonhtmldoc">doc xml xls rtf ppt:pps ps:eps mso tex dsc asc log rdf dvi docbook sgml txt odt ods</group>
                <group label="audio">ram:ra:rm mp3 wma mid pls:m3u wav asf au</group>
                <group label="video">avi qt wmv mov mpg:mpeg</group>
                <group label="software">exe rpm diff patch iso deb:debian:udeb pdb:prc jar</group>
                <group label="source">h c cc:cpp java:jav sh glade in ada</group>
                <group label="compress">zip gz tar:tgz z lhz:lha rar hqx sit bz2:bzip2</group>
            </log>

            <!-- Files with these extensions mut be counted and nothing else. The count includes
                 the extension and the number of times it appeared in the collection, nothing else.
                 The items will be counted only if they are in the same server as the originating
                 page, so an embedded image or script from a remote server doesn't count.-->

            <stat>
                <group label="perfhashstaticrandoms">0 0 0</group>
                <group label="extras">css js swf:fla</group>
                <group label="image">gif jpg:jpeg:jpe png svg bmp:pcx ico tiff:tif wmf img pbm:xbm:xpm</group>
            </stat>

            <!-- Files with these extensions should be silently ignored -->
            <ignore>
                <group label="perfhashstaticrandoms">0 0 0</group>
                <group label="ignore">NONE</group>
            </ignore>


        </extensions>

        <!-- To avoid indexing the session-id embedded in the URL,
             all strings of the form '[alphanum]+sessionid=[alphanum]+'
              will be removed from the URL -->
        <sessionids>sessionid SESSID session sess_id session_id session-id RefererURL MSCSID CartID CFID CFTOKEN POSTNUKESID eZSESSIDpages SESSIDGE</sessionids>

        <!-- Patterns to reject, URLs containing these patterns 
             won't be added to the crawler's queue.

             The idea is to add common patterns of pages that
             carry no useful information -->
             
             <reject-patterns>/archive =view_day =view_month =view_year =Calendar =viewprofile appointmentedit mailtofriend articleprint action=reporttm =Profile =Post =Login Members =Online =Reg =Stats product_reviews_write PostCalendar =friend SPChat userinfo login logout FriendSend Your_Account Feedback Guestbook =print =Cart webalizer stats/usage showcounter</reject-patterns>

        <!-- Automatically add base URLs to sites, i.e.: if we see for
             the first time a link of the form www.example.com/a.html,
             it will also add the home page www.example.com
             Default: 1 -->

        <add-root>1</add-root>

        <!-- Check for robots.txt files
             Default: 1 (dont' change this: it is important to respect
             the robot exclusion protocol) -->
        <add-robots-txt>1</add-robots-txt>

        <!-- Check for sitemap.rdf files, this was an experimental feature.
             Default: 0-->
        <add-sitemap-rdf>0</add-sitemap-rdf>
 
        <!-- Check for sitemap.xml files, this was an experimental feature.
             Default: 0-->
        <add-sitemap-xml>0</add-sitemap-xml>

        <!-- Enable feeds xml reading, this was an experimental feature.
             Default: 0-->
        <add-feeds-xml>0</add-feeds-xml>
 
        <!-- Remove the source harvester files to save space=1,
             Keep the files=0, useful with verbose logging for debugging -->
        <removesource>0</removesource>

    </seeder>

    <!-- Parameters for the manager program.
         The manager program generates batches of documents
         to be downloaded by the harvester, based on a function
         of the metadata.
     -->
    <manager>

        <!-- This is the maximum depth at which a document will
             be harvested. It is different for dynamic and static pages,
             because the number of dynamic pages at a certain depth
             tends to grow exponentially -->
        <maxdepth>

            <!-- Pages identified as dynamic, either by having a question
                 mark or by using a filename extension known as dynamic.
                 Default: 5 levels (level 1 is the home page)-->
            <dynamic>15</dynamic>

            <!-- Other pages
                 Default: 15-->
            <static>20</static>
        </maxdepth>


        <batch>

            <!-- Maximum number of documents assigned per batch. Each time the manager
                 runs, it will generate 'count' (see below) batches of this amount of documents.
                 Currently the batch should fit in main memory.
                 For a 1GB machine the maximum batch size is about 200K-300K pages.
                 Check the memory usage of the harvester and the manager and if they are
                 OK, keep increasing this until you're using about 80% of the memory. -->
            <size>1200K</size>

            <!-- Maximum number of documents from the same site in
                 each batch. If it is set too high, the harvesters
                 will get stuck in a few websites.

                 This is closely related to the number of seconds between
                 accesses, i.e.: if you are going to download 500 documents
                 from a server, each 15 seconds, you have a minimum
                 duration of 2 hours for each batch.
                 Default: 500-->
             <samesite>500</samesite>

            <!-- Number of batches to assign in each round; this means, how many times
                 the 'harvester' program has to be run after each 'manager' execution;
                 It is useful by the end of a large crawl to run several (say, 2, 5, or maybe even 10)
                 harvester rounds after each manager, as this saves time. Note that this is not equivalent
                 to increase the batch size, as the latter requires more memory to hold the
                 batch in memory.
                 Note also that it is likely that at the begining the manager will
                 have enough sites for 1 batch only.
                 Default: 1 -->
            <count>1</count>

        </batch>

        <!-- Parameters for handling errors in sites -->

        <max-errors>
            <!-- Maximum number of CONSECUTIVE errors of the
                 same site, in DIFFERENT batches,
                 over this the websites are not re-scheduled.
                 Default: 5 -->
            <different-batch>5</different-batch>

            <!-- Maximum number of CONSECUTIVE errors of the
                 same site, allowed during the SAME harvest,
                 over this, the rest of the pages are skipped,
                 and the site is re-scheduled for the next
                 harvest batch.
                 Default: 1 (meaning two consecutive errors will
                 move the rest of the pages to the new batch)-->
            <same-batch>1</same-batch>
        </max-errors>

        <!-- The score of a page is the value of this page for the
             collection. The manager tries to maximize the value of
             the collection, considering the following function:

              current_score =
               sum(scores * weights) * (outdated_probability ^ outdatedexpon)

             When the document is downloaded, the probability that
             the document is outdated is zero, so:

                future_score = current_score [with outdatedprob=0]

             What the manager does is select the set of pages with
             maximum (future_score - current_score), to maximize
             the "earnings" in each batch.

        -->

        <score>

            <!-- Periodic execution instead of massive elaborations -->
            <periodic>

                <!-- Enabled on/off 1/0 -->
                <enabled>0</enabled>

                <!-- Best moment of the day in hours - 0 % 24 UTC - when start ranking -->
                <best-moment>0</best-moment>

                <!-- Ranking will be executed every number of days - max 28, default 1 - -->
                <interval>7</interval>

                <!-- Default siterank for new site before ranking -->
                <default-siterank>0.1</default-siterank>

                <!-- Default pagerank for new documents from new site before ranking -->
                <default-pagerank>0.1</default-pagerank>

                <!-- Default wlscore for new documents from new site before ranking -->
                <default-wlscore>0.1</default-wlscore>

                <!-- Default hub for new documents from new site before ranking -->
                <default-hub>0.1</default-hub>

                <!-- Default autority for new documents from new site before ranking -->
                <default-authority>0.1</default-authority>

                <!-- Default liverank for new documents from new site before ranking -->
                <default-liverank>1.0</default-liverank>

            </periodic>

            <!-- Factor -->
            <factor>

                <!-- Factor if domains are pippo.sitename.cxm and pluto.sitename.cxm -->
                <domain>10</domain>

                <!-- Factor if domains are *.sitename.cxm and *.sitename2.cxm or
                     *.sitename.cxm and *.sitename.com -->
                <external>20</external>

                <!-- Factor if domains are equals -->
                <fqdn>18</fqdn>

            </factor>

            <!-- Pagerank -->
            <pagerank>

                <!-- Importance for the scheduling -->
                <weight>1</weight>

                <!-- This is the decay parameter for the iteration, typical
                     values are 0.9 and 0.85 -->
                <dampening>0.85</dampening>

                <!-- Maximum average relative error.
                    In collections in the order of 2-4 million pages:
                         0.01 - about 20 iterations
                        0.005 - about 30 iterations
                        0.001 - about 70 iterations

                    -->
                <max-error>0.01</max-error>

            </pagerank>

            <!-- Weighted link score -->
            <wlscore>

                <!-- Importance for the scheduling -->
                <weight>1</weight>
            </wlscore>

            <!-- HITS algorithm -->
            <hits>

                <!-- Importance of hub score for scheduling -->
                <hub>
                    <weight>1</weight>
                </hub>

                <!-- Importance of authority score for scheduling -->
                <authority>
                    <weight>1</weight>
                </authority>

                <!-- Maximum average relative error.
                    In collections in the order of 2-4 million pages:
                         0.005 - about 4 iterations
                        0.001 - about 10-20 iterations
                        0.0005 - about 30-50 iterations
                    -->
                <max-error>0.001</max-error>
            </hits>

            <!-- liverank -->
            <liverank>

	        	<!-- If true, then also keep a local sensitivy hashing sketch for
	             each page. The contents of the pages are not changed, the
    	         sketches are stored in a separate file. Later you can read
        	     the sketches using cbot-info-extract -->

                <!-- Importance for the scheduling -->
                <weight>1</weight>

                <!-- Number of visits to sum at real number,
                     200 can be a good value -->
                <starting-visits>200</starting-visits>

                <!-- Minimum permutation in LSH sketch to consider,
                     a decent change of pages -->
                <min-sketch>20</min-sketch>

            </liverank>

            <!-- The siterank is like pagerank, but related to the
                 graph of sites, in which multiple links between
                 pages on different sites are collapsed to a single
                 link with a given weight -->

            <siterank>

                <!-- Importance of siterank -->
                <weight>1</weight>

                <!-- Maximum average relative error, as the graph of
                websites is much smaller than the graph of documens,
                this converges much faster and can be much accurate -->
                <max-error>0.000001</max-error>

            </siterank>

            <!-- Importance of a page being static, static pages are
                 given priority over dynamic pages.
                 If static=true, this amount of score is added -->
            <static>
                <weight>0</weight>
            </static>

            <!-- Importance of depth of a page.
                 If a page is at maximum depth, 0 points are added to the score
                 If a page is at depth = 1, this amount of points is added
                 Other depths are assigned scores lineary -->
            <depth> 
                <weight>95</weight>
            </depth>

            <!-- Importance of the size of the site in which the page
                 resides, only considering remaining pages -->
            <queue-size>
                <weight>0</weight>
            </queue-size>

            <!-- A small, random number of points is added to the pages, to 
                 avoid selecting always the same pages -->
            <random>
                <weight>5</weight>
            </random>

            <!-- This exponent controls how important is the age of a page
                 for the crawler. Normally it should be set to 1 -->
            <age>
                <exponent>1</exponent>
            </age>

            <!-- To ensure that at least the front page of each website is
                 downloaded, the first page of each site can have its score
                 set to the highest value possible. 0=off, 1=on. -->
            <boostfrontpage>1</boostfrontpage>
        </score>

        <!-- Minimum age to visit a page, in seconds
             1 day = 24 hr * 60 min * 60 seg = 86400 seg
             1 week = 7 * 1 day = 604800
             1 month ~ 30 * 1 day = 2592000 -->
        <minperiod>
            <!-- Feeds forbidden by robots.txt rules-->
            <feeds-forbidden>60</feeds-forbidden>

            <!-- If the page of xml feeds had an error, i.e.: time out, or
                 DNS error, or connection failed to the server -->
            <feeds-unsuccessful>20</feeds-unsuccessful>
            
            <!-- If the page of xml feeds could be retrieved, but it was not ok,
                 i.e.: 404 errors, server errors, etc -->
            <feeds-successful-but-not-ok>20</feeds-successful-but-not-ok>
            
            <!-- If the page of xml feeds was OK or redirect (code 20x) -->
            <feeds-successful-and-ok>20</feeds-successful-and-ok>

            <!-- Forbidden by robots.txt rules-->
            <forbidden>60</forbidden>

            <!-- If the page had an error, i.e.: time out, or
                 DNS error, or connection failed to the server -->
            <unsuccessful>60</unsuccessful>
            
            <!-- If the page could be retrieved, but it was not ok,
                 i.e.: 404 errors, server errors, etc -->
            <successful-but-not-ok>60</successful-but-not-ok>
            
            <!-- If the page was OK or redirect (code 20x) -->
            <successful-and-ok>60</successful-and-ok>
            
            <!-- If the page was OK or redirect (code 20x) and liverank enable.
                 This limit must be inferior at 'successful-and-ok'
                 and is need to tell at crawler that
                 cannot be overcome in any case. 
                 You can have 'successful-and-ok' set to 1 week
                 but crawler through internal algorithm can reduce it to
                 the minimum 'extreme-successful-and-ok' value.
                 To disable this functionality, leave it to '0'. -->
            <extreme-successful-and-ok>30</extreme-successful-and-ok>

            <!-- Query robots.txt file -->
            <robots-txt>
                <when-valid>1w</when-valid>
                <when-not-valid>4w</when-not-valid>
            </robots-txt>
            
            <!-- Query sitemap.rdf file -->
            <sitemap-rdf>
                <when-valid>12h</when-valid>
                <when-not-valid>4w</when-not-valid>
            </sitemap-rdf>
            
            <!-- Query sitemap.xml file -->
            <sitemap-xml>
                <when-valid>12h</when-valid>
                <when-not-valid>4w</when-not-valid>
            </sitemap-xml>

        </minperiod>

    </manager>

    <!-- Parameter for the harvester program.
         The harvester downloads pages from the web, according to
         a batch that is given by the manager -->

    <harvester>

        <!-- These are the contents of /etc/resolv.conf, and
             are used by the asynchronous dns interface; use
             several 'nameserver' lines, and no 'search' line.

             If you are crawling a top level domain, say, .ZZ, you
             should check first with the nameservers of that domain,
             do "dig gr @F.ROOT-SERVERS.NET" to get this list,
             and TEST this first, make sure there are no dead
             DNSs in your list-->
        <resolvconf>nameserver 82.85.180.70
			        nameserver 151.12.122.182</resolvconf>

        <!-- The path of Certification Authority file in pem format.
             The path depend on the Linux distribution you use.
             Check if it is rigth and work.
             If crawler don't support https protocol it will be ignored -->
        <pemca>/etc/pki/tls/cert.pem</pemca>

        <!-- Blocked IP addresses (perfect hashing) -->
        <blocked-ip>
                <group label="perfhashstaticrandoms">0 0 0</group>
                <group label="ip">192.168.0.1</group>
		</blocked-ip>

        <!-- User agent. If here, string 'zzz' is given, then the
             used user-agent will be
                CBOT/version (sysname; machine; zzz)
             example: "CBOT/1.0 (Linux; i686; zzz )".
             I recommend to put an e-mail address or a Web page for
             your project here -->
        <user-agent-comment>alt</user-agent-comment>

        <!-- Number of simultaneous download -->
        <nthreads>

            <!-- Starting threads. If the linux threads implementation
                 is used, you cannot go past 400 threads; start with
                 a safe value of 200 and then increment gradually to see
                 how much your machine can take.

                 The usual setting is about 1000.
                 The default 300 is just a failsafe limit.

                 It requires a tiny bit of tunning of the system
                 parameters (via the files in /proc/sys/fs in Linux,
                 or by setting user limits to descriptors)
                 to use this at full capacity, i.e.: more than 700 threads -->

            <start>300</start>

            <!-- Minimum threads, soft limit. If the harvester has less than
                 this ammount of threads running, it should stop, unless
                 less than softmintime have passed. This scheme of soft
                 and hard minimum is to avoid having to change this setting
                 by the end of a crawl -->
            <softmin>15</softmin>

            <softmintime>1h</softmintime>

            <!-- Minimum threads, hard limit. If the harvester has less
                 than this amount of threads running, it MUST stop now;
                 the harvester will not start successfully if less than
                 this amount of sites is given.

                 Note that if, e.g.: hardmin is 3, then you cannot crawl
                 less than 3 sites  -->
            <hardmin>0</hardmin>

        </nthreads>

        <!-- Several timeouts for downloading pages, in seconds -->
        <timeout>
            <!-- Timeout for the connect() call -->
            <connection>10</connection>

            <!-- Timeout for read() and write() -->
            <readwrite>30</readwrite>

            <!-- Poll period, in milliseconds, it must be greater
                 than zero -->
            <poll>2000</poll>

            <!-- This is the minimum speed for downloading data. If
                 less than x bytes/seconds are downloaded, the download
                 timeouts.
                 
                 NOT IMPLEMENTED YET in the non-threads implementation. -->
            <bytes-per-second>400</bytes-per-second>
        </timeout>

        <!-- Maximum filesize in bytes, no more than this amount
             of bytes is downloaded. A "Range:" header is added to the
             request, but if the server doesn't honor the limit, the
             connection is dropped by the crawler.

             400Kb is a rather safe default,
             100Kb is ok if you want to save disk space, BUT there will be
                some false positives in the duplicate detection  -->
        <maxfilesize>1000K</maxfilesize>

        <!-- Maximum number of simultaneous DNS queries.-->
        <dnsmax>40</dnsmax>

        <!-- Timeout for the DNS resolver, in seconds. Don't set this
             to low to avoid too many DNS errors. MAX 10 seconds -->
        <dnstimeout>10</dnstimeout>

        <!-- The crawler tries to be "polite" with servers. It will 
             wait between accesses to the same site, and a site will
             always be assigned to only one harvester at a time -->
        <wait>
            <!-- Seconds to wait between download of pages -->
            <normal>2</normal>

            <!-- For "big" websites, this time can be reduced. This is
                 the number of pages that a website has to be
                 considered "big" -->
            <countbig>60</countbig>

            <!-- Seconds to wait between downloads in a big website -->
            <big>5</big>

        </wait>

        <!-- Mime types to accept, separated by COMMAS. These mime-types
             must be accepted by the parser in the gatherer program -->
        <acceptmime> text/html,text/plain,application/x-httpd-php,application/pdf</acceptmime>

        <!-- Language to accept, separated by COMMAS. These languages
             can be be accepted by the gatherer program -->
        <acceptlanguage> it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3,*;q=0.1</acceptlanguage>

        <!-- Enconding to accept, separated by COMMAS. These encoding
             must be accepted by the gatherer program -->
        <acceptencoding> gzip,deflate</acceptencoding>

        <!-- Verbose logging: 0=no log, 1=normal, 2=verbose -->
        <loglevel>0</loglevel>

        <!-- DNS addresses are cached on disk, regardless
             of their expiration date -->
        <dnsexpire>4w</dnsexpire>

	<!-- Max number of redirect in same cicle of harvesting
	     for links.-->
	<maxattempts>3</maxattempts>

    </harvester>

    <!-- Parameters for the gatherer program.
         The gatherer program parses the pages downloaded by the
         harvester, adding them to the collection, and obtains urls
         for the seeder -->
         
    <gatherer>

        <!-- By default, all tags are discarded, and all text is kept,
             these parameters are the exceptions to this rule -->

        <discard>
            <!-- The textual content of these tags will be discarded,
                 (the tags themselves are always discarded) (perfect hashing) -->
            <content>
                <group label="perfhashstaticrandoms">0 0 0</group>
                <group label="content">style script noscript object embed select noframes</group>
			</content>
        </discard>

        <keep>
            <!-- These tags will be preserved (its text is always preserved) (perfect hashing);
                 META/A/LINK/HREF/HTTP-EQUIV/CONTENT/FRAME/AREA/ALT must
                 be kept because they are used to search for links;
                 If you want to save more disk space, use this:
                 'a meta title frame link img area embed' -->
            <tag>
                <group label="perfhashstaticrandoms">0 0 0</group>
                <group label="tags">a body br p meta title frame link img area embed table td div h1 h2 h3 h4 h5 h6 ul font span i</group>
			</tag>
            <attribute>
                <group label="perfhashstaticrandoms">0 0 0</group>
                <group label="attributes">style http-equiv href src alt description keywords name content rel title</group>
			</attribute>
        </keep>

        <!-- These parameters tell the gatherer where to look for
             links to other files -->
        <link>
            
            <!-- This is a list of
                   tag/attribute/anchor-text[+]
                 tag is the name of the tag that contains a link
                 attribute is the attribute containing the url of the link
                 anchor-text is the attribute containing the description of the link
                 a plus sign at the ends means that the content of the
                 tag is also considered anchor text -->
            <attribute>frame/src/name a/href/title+ area/href/alt img/src/alt link/href/rel embed/src/name</attribute>
		</link>

        <domain>
            <!-- File contenente i domini che NON devono essere indicizzati. -->
            <blacklist>/tmp/cbotdata/black_urls.txt</blacklist>

	        <!-- Se vero, abilita le access list per i domini.
	             Per essere indicizzati devono contenere una parola chiave (goodwords) o essere contenuti nel file 'domains-whitelist'.
    	         default: false (0) -->
        	<use-whitelist>1</use-whitelist>

            <!-- File contenente i domini che devono essere indicizzati in modalità 'use-domains-whitelist'. -->
            <whitelist>/tmp/cbotdata/white_urls.txt</whitelist>

           <!-- In modalità 'use-domains-whitelist', la presenza di una solo di queste parole chiave nel nome di dominio,
                (es: sardegna nel dominio www.sardegnacavalli.it) ne permette la ricezione a prescindere.
				In pratica tutti i domini che contengono una di queste parole chiave verranno salvati.
				Ovviamente il dominio di primo livello deve essere tra quelli accettabili.-->
            <goodwords>sardegna sardinia shardana barbagia campidano gallura marmilla ogliastra nuraghe sartiglia tharros cagliari olbia oristano nuoro sassari arbatax olbia .ca.it .ci.it .ot.it .og.it .or.it .ot.it .ss.it .nu.it .vs.it</goodwords>
        </domain>

        <!-- The gatherer can detect dynamic documents under static pages.
             If a document changes allways, after x visits, then it will
             be considered dynamic -->
        <changetodynamic>3</changetodynamic>

        <!-- Maximum size, in bytes, of the data that is kept after
             parsing the document; don't set this too low (e.g.: 1000),
             because the data that is saved is used to check for duplicates,
             and many pages start with the same characters. A good
             minimum could be 30K; usually HTML files are 40k-50k and
             are reduced to 15k-25k by the parser -->
        <maxstoredsize>50K</maxstoredsize>

        <!-- If true, then store only a hash function of the pages, this
             is useful to preserve bandwidth and still prevent duplicates,
             if you are not interested in the actual page contents;
             Note that you still must set maxstoredsize to a reasonable
             value (say, at least 30K) because the hash function is
             calculated after parsing;
             default: false (0) -->
        <save-hash-only>0</save-hash-only>

        <!-- If true, then also keep a local sensitivy hashing sketch for
             each page. The contents of the pages are not changed, the
             sketches are stored in a separate file. Later you can read
             the sketches using cbot-info-extract
             default: false (0) -->
        <use-sketches>1</use-sketches>

        <!-- If true, then convert all documents to UTF-8;
             Note that doc.charset will continue to be the original charset,
             so during the analysis you still can get statistics on the
             charsets in the collection; if you get some error like
             'Invalid or incomplete multibyte or wide character', set
             this to zero.
             default: true (1) -->
        <convert-to-utf8>1</convert-to-utf8>

        <!-- Default charset for pages in which the charset is not specified
             and cannot be detected. The default config is "ISO-8859-1"
             as it is the most used in western europe and latin america,
             and it does not hurt pages written in english; if your country uses
             typically a different charset, change this setting. -->
        <defaultcharset>ISO-8859-1</defaultcharset>

    </gatherer>

    <!-- Indexer parameters -->
    <index>
        <connector>

             <basepath>/engine/connectors</basepath>

             <!-- Path where unix sockets are found -->.
             <sockets-basepath>/engine/connectors/sockets</sockets-basepath>

             <!-- Prefix of filename socket for infix connection instance.
                  Next in the programs the filename will be completed with double dot and number like: filenamesocket:3 -->
             <infix-sock-prefix>infix_socket</infix-sock-prefix>

             <!-- Network engine ip. 
                  If not, it must be set as 127.0.0.0 -->
             <engine-network-mask>127.0.0.0</engine-network-mask>
        </connector>

        <!-- Size of the description (text extract) for the indexing
             process, in bytes -->
        <descriptionsize>200</descriptionsize>

        <!-- File with stopwords. These stopwords are discarded to preserve
             space in the index -->
        <stopwordsfile>/usr/local/share/CBOT/lang/english</stopwordsfile>

        <!-- Minimum and maximum length of words -->
        <minwordlength>5</minwordlength>
        <maxwordlength>20</maxwordlength>
    </index>

    <!-- Analysis -->
    <analysis>
        <!-- This is the name of the collection, and will be used
             in report titles and output filenames. It must not
             have spaces. Useful titles are, e.g.: CL2004FEB,
             GR2005MAR, etc. -->
        <code>GravacaoDeArquivos230609</code>

        <!-- Language analysis is done using stopwords. 
            Notice that two stopword files CANNOT have
            interferences (they must be cleaned first to remove
            all common stopwords)

        -->
        <lang>
                <basepath>/usr/local/share/CBOT/lang</basepath>

                <!-- The language files should be listed starting with the
                     one that is more common in the collection and files must be renamed
                     as ISO 639-1 Language Codes.
                     english (en),
                     italian (it),
                     french (fr),
                     spanish (es),
                     catalan (ca),
                     portuguese (pt) -->
                <stopwords>en it fr es ca pt</stopwords>

                <!-- Take a sample of 1 every X documents to check
                     for its language. -->
                <sample-every>1</sample-every>

                <!-- Take a sample of K document texts on each language,
                     to see how good/bad are the language files -->
                <save-text>40</save-text>

                <!-- Minimum number of words in the document,
                less than this implies undefined language -->
                <min-words>50</min-words>

                <!-- Minimum number of stopwords of a language
                to have the possibility of be considered in that
                language -->
                <min-stopwords>15</min-stopwords>

                <!-- Difference in the number of stopwords between
                the first and the second place, to be considered
                as this language -->
                <min-difference>12</min-difference>

        </lang>

        <maxyears>
            <tables>10</tables>
            <graphs>5</graphs>
        </maxyears>

        <!-- Below, fitting parameters fitmin and fitmax are two
             numbers indicating the X range in which gnuplot is
             expected to FIT the data.

             Set fitmin=fitmax=0 to avoid fitting data.

             Range, on the contrary, is the X and Y range on
             which to PLOT data, and is given in the gnuplot
             format '[...] [...]', in which the first pair of brackets
             enclose the X range and the second pair of brackets
             encloses the Y range.

             The default '[] []' includes all points in the X and Y
             ranges. You might want to change this to improve graphics,
             after changing, just re-run the corresponding cbot-report-*
             to re-generate the graph -->


        <!-- Document report -->
        <doc>

            <!-- Fraction of sampled document for the
                 scatter graphs -->

            <sample-every>300</sample-every>

            <!-- Graph with a histogram of page age in days,
                 short period -->

            <age_days_small>
                <range>[1:120]</range>
            </age_days_small>

            <!-- Graph with a histogram of page age in days,
                 long period -->

            <age_days_large>
                <range>[1:720]</range>
            </age_days_large>

            <!-- Graph with the histogram of parsed content length -->

            <content_length_kb>
                <fitmin>20</fitmin>
                <fitmax>100</fitmax>
                <range>[] []</range>
            </content_length_kb>

            <!-- Graph with the histogram of unparsed content length -->

            <raw_content_length_kb>
                <fitmin>30</fitmin>
                <fitmax>200</fitmax>
                <range>[] []</range>
            </raw_content_length_kb>

            <!-- Graph with the histogram of in-degree of documents -->

            <in_degree>
                <fitmin>10</fitmin>
                <fitmax>1000</fitmax>
                <range>[] []</range>
            </in_degree>

            <!-- Graph with the histogram of out-degree of documents -->

            <out_degree>
                <fitmin>60</fitmin>
                <fitmax>120</fitmax>
                <range>[] []</range>
            </out_degree>

            <!-- Graph with the histogram of pagerank -->

            <pagerank>
                <fitmin>5e-6</fitmin>
                <fitmax>5e-5</fitmax>
                <range>[] []</range>
            </pagerank>

            <!-- Graph with the histogram of weighted link rank -->

            <wlrank>
                <fitmin>5e-6</fitmin>
                <fitmax>5e-5</fitmax>
                <range>[] []</range>
            </wlrank>

            <!-- Graph with the histogram of hub score -->

            <hubrank>
                <fitmin>1e-6</fitmin>
                <fitmax>1e-4</fitmax>
                <range>[] []</range>
            </hubrank>

            <!-- Graph with the histogram of authority score -->

            <authrank>
                <fitmin>1e-6</fitmin>
                <fitmax>1e-4</fitmax>
                <range>[] []</range>
            </authrank>

        </doc>

        <!-- Sites report -->
        <site>

            <!-- Number of sites to show in the list of top sites
                 by number of pages, or size, etc. -->

            <site_top>
                <cut>80</cut>
            </site_top>

            <!-- Fraction of sites to include in the scatter plots.
                 Only 1 out of X will be included. -->

            <scatter-plot-every>10</scatter-plot-every>

            <!-- Graph with the histogram of the
                 sum of raw content length of the pages in the website -->

            <site_raw_content_length_mb>
                <fitmin>1</fitmin>
                <fitmax>100</fitmax>
                <range>[ ] []</range>
            </site_raw_content_length_mb>

            <!-- Graph with the histogram of the 
                 number of documents per website -->

            <site_count_doc>
                <fitmin>50</fitmin>
                <fitmax>500</fitmax>
                <range>[ ] []</range>
            </site_count_doc>

            <!-- Graph with the histogram of the number of
                 different sites linking to each site -->

            <site_in_degree>
                <fitmin>10</fitmin>
                <fitmax>100</fitmax>
                <range>[ ] []</range>
            </site_in_degree>

            <!-- Graph with the histogram of the number of different
                 sites linked by each site -->

            <site_out_degree>
                <fitmin>10</fitmin>
                <fitmax>100</fitmax>
                <range>[ ] []</range>
            </site_out_degree>

            <!-- Graph with the histogram of the number of internal links 
                 in each site -->

            <site_internal_links>
                <fitmin>100</fitmin>
                <fitmax>1000</fitmax>
                <range>[ ] []</range>
            </site_internal_links>

            <!-- Graph with the histogram of the number of internal
                 links divided by the number of documents in each site,
                 i.e.: the average internal links per document in each site -->

            <site_internal_links_by_ndocs>
                <fitmin>8</fitmin>
                <fitmax>30</fitmax>
                <range>[ ] []</range>
            </site_internal_links_by_ndocs>

            <!-- Graph with the histogram of the sum of the Pagerank
                 over all documents in the site -->
            
            <sum_pagerank>
                <fitmin>1e-6</fitmin>
                <fitmax>1e-4</fitmax>
                <range>[ ] [ ]</range>
            </sum_pagerank>

            <!-- Graph with the histogram of the sum of the hub score
                 over all documents in the site -->

            <sum_hubrank>
                <fitmin>0</fitmin>
                <fitmax>0</fitmax>
                <range>[ ] [ ]</range>
            </sum_hubrank>

            <!-- Graph with the histogram of the sum of the authority score
                 over all documents in the site -->

            <sum_authrank>
                <fitmin>0</fitmin>
                <fitmax>0</fitmax>
                <range>[ ] [ ]</range>
            </sum_authrank>

            <!-- Graph with the histogram of the siterank of the site -->

            <siterank>
                <fitmin>1e-6</fitmin>
                <fitmax>1e-4</fitmax>
                <range>[ ] [ ]</range>
            </siterank>


        </site>

        <!-- Sitelink report -->
        <sitelink>

            <!-- Graph with the histograph of the number of sites per
                 strongly connected component -->

            <scc_sizes>
                <fitmin>2</fitmin>
                <fitmax>10</fitmax>
                <range>[ ] []</range>
            </scc_sizes>
        </sitelink>

        <!-- Extensions report -->
        <extension>

            <tld>
                <!-- File containing an estimation on the size of top
                     level domains. The format is one row per domain.
                     On each row, the name of the tld, a comma, and
                      the estimation of the size -->

                <sizes-file>/usr/local/share/CBOT/tld/sizes.csv</sizes-file>

                <!-- Minimum fraction of links to a country to be considered
                     in the list of linked countries by over-representation,
                     or under-representation. This is to avoid showing
                     domains that are too small. -->

                <min-links-difference>1e-6</min-links-difference>
            </tld>

            <extensions>
                <cut>120</cut>
            </extensions>

            <!-- Maximum number of top level domains to show in the reports.
                 Only the top N will be shown. -->

            <top_level_domains>
                <cut>170</cut>
            </top_level_domains>

        </extension>

    </analysis>

</config>
